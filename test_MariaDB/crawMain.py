# crawMain.py ver1.0
import requests
from bs4 import BeautifulSoup
import mysql.connector

# âœ… MariaDB ì ‘ì† ì„¤ì •
config = {
    "host": "localhost",
    "user": "",
    "password": "",
    "database": "bcw",
    "port": 3306
}

# âœ… DB ë° í…Œì´ë¸” ìë™ ìƒì„±
def initialize_mariadb():
    try:
        conn = mysql.connector.connect(
            host=config["host"],
            user=config["user"],
            password=config["password"],
            port=config["port"]
        )
        cur = conn.cursor()
        cur.execute("CREATE DATABASE IF NOT EXISTS bcw")
        conn.commit()
        cur.close()
        conn.close()

        conn = mysql.connector.connect(**config)
        cur = conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS posts (
                id INT AUTO_INCREMENT PRIMARY KEY,
                title VARCHAR(255),
                author VARCHAR(255),
                date VARCHAR(255),
                keywords TEXT,
                summary TEXT,
                UNIQUE(title, author, date)
            )
        """)
        conn.commit()
        cur.close()
        conn.close()

        print("âœ… MariaDB ì´ˆê¸°í™” ì™„ë£Œ (DB + í…Œì´ë¸”)")
    except Exception as e:
        print("âŒ DB ì´ˆê¸°í™” ì‹¤íŒ¨:", e)

# âœ… í¬ë¡¤ë§ëœ ê²°ê³¼ë¥¼ MariaDBì— ì €ì¥
def save_to_mariadb(results):
    inserted = []
    try:
        conn = mysql.connector.connect(**config)
        cur = conn.cursor()

        for post in results:
            try:
                cur.execute("""
                    INSERT INTO posts (title, author, date, keywords, summary)
                    VALUES (%s, %s, %s, %s, %s)
                    ON DUPLICATE KEY UPDATE
                        keywords = VALUES(keywords),
                        summary = VALUES(summary)
                """, (
                    post["ì œëª©"],
                    post["ì‘ì„±ì"],
                    post["ë‚ ì§œ"],
                    post["í‚¤ì›Œë“œ"],
                    post["ë³¸ë¬¸ ìš”ì•½"]
                ))
                if cur.rowcount == 1:
                    inserted.append(post)
            except Exception as e:
                print("âŒ ì‚½ì… ì˜¤ë¥˜:", e)

        conn.commit()
        cur.close()
        conn.close()
        print("âœ… MariaDB ì €ì¥ ì™„ë£Œ")
        return inserted

    except Exception as e:
        print("âŒ DB ì—°ê²° ì‹¤íŒ¨:", e)
        return []

# âœ… í¬ë¡¤ë§ ì‹¤í–‰
def crawl_bcw_data():
    proxies = {
        "http": "socks5h://127.0.0.1:9050",
        "https": "socks5h://127.0.0.1:9050"
    }

    base_url = "http://bestteermb42clir6ux7xm76d4jjodh3fpahjqgbddbmfrgp4skg2wqd.onion/"
    forum_path = "viewforum.php?f=30"
    results = []

    for page in range(10):
        start = page * 25
        url = f"{base_url}{forum_path}" if page == 0 else f"{base_url}{forum_path}&start={start}"
        print(f"ğŸ“„ í˜ì´ì§€ {page+1} ìš”ì²­ ì¤‘: {url}")

        try:
            res = requests.get(url, proxies=proxies, timeout=20)
            soup = BeautifulSoup(res.text, "html.parser")
            post_links = soup.select("a.topictitle")

            for post in post_links:
                title = post.text.strip()
                href = post.get("href")
                if not href:
                    continue
                post_url = base_url + href.lstrip("./")

                try:
                    post_res = requests.get(post_url, proxies=proxies, timeout=20)
                    post_soup = BeautifulSoup(post_res.text, "html.parser")

                    author_node = post_soup.select_one("p.author strong")
                    author = author_node.get_text(strip=True) if author_node else "(ì‘ì„±ì ì—†ìŒ)"

                    author_tag = post_soup.select_one("p.author")
                    date = author_tag.get_text(strip=True).split("Â»")[-1].strip() if author_tag else "(ë‚ ì§œ ì—†ìŒ)"

                    body_tag = post_soup.select_one("div.postbody")
                    summary = body_tag.get_text(strip=True)[:200] if body_tag else "(ë³¸ë¬¸ ì—†ìŒ)"

                    keywords = []
                    for kw in ["USA", "database", "customer", "SSN", "DOB", "fullz", "MEGA", "BIN", "email", "phone", "txt", "leak"]:
                        if kw.lower() in summary.lower():
                            keywords.append(kw)

                    results.append({
                        "ì œëª©": title,
                        "ì‘ì„±ì": author,
                        "ë‚ ì§œ": date,
                        "í‚¤ì›Œë“œ": ", ".join(keywords),
                        "ë³¸ë¬¸ ìš”ì•½": summary
                    })

                except Exception as e:
                    results.append({
                        "ì œëª©": title,
                        "ì‘ì„±ì": "(ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨)",
                        "ë‚ ì§œ": "-",
                        "í‚¤ì›Œë“œ": "-",
                        "ë³¸ë¬¸ ìš”ì•½": f"(ë³¸ë¬¸ ë¡œë”© ì‹¤íŒ¨: {e})"
                    })

        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page+1} ìš”ì²­ ì‹¤íŒ¨: {e}")

    new_posts = save_to_mariadb(results)
    if new_posts:
        print# í†µí•©ëœ bcw_crawler_with_telegram.py

import requests
from bs4 import BeautifulSoup
import mysql.connector
import json

# âœ… MariaDB ì ‘ì† ì„¤ì •
config = {
    "host": "localhost",
    "user": "root",            # â† ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ë³€ê²½
    "password": "1234",        # â† ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ë³€ê²½
    "database": "bcw",
    "port": 3306
}

# âœ… í…”ë ˆê·¸ë¨ ì„¤ì •
bot_token = "7904703336:AAGnhRjYO42TIF6VuP-sCVfZMZ_OjToPxuI"
chat_id = "7894265954"

def send_message(message):
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    data = {"chat_id": chat_id, "text": message}
    try:
        res = requests.post(url, data=data)
        if res.status_code == 200:
            print("âœ… í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ")
        else:
            print(f"âŒ í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {res.status_code}")
    except Exception as e:
        print(f"âŒ í…”ë ˆê·¸ë¨ ì „ì†¡ ì˜ˆì™¸: {e}")

# âœ… MariaDB ì´ˆê¸°í™”
def initialize_mariadb():
    try:
        conn = mysql.connector.connect(
            host=config["host"],
            user=config["user"],
            password=config["password"],
            port=config["port"]
        )
        cur = conn.cursor()
        cur.execute("CREATE DATABASE IF NOT EXISTS bcw")
        conn.commit()
        cur.close()
        conn.close()

        conn = mysql.connector.connect(**config)
        cur = conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS posts (
                id INT AUTO_INCREMENT PRIMARY KEY,
                title VARCHAR(255),
                author VARCHAR(255),
                date VARCHAR(255),
                keywords TEXT,
                summary TEXT,
                UNIQUE(title, author, date)
            )
        """)
        conn.commit()
        cur.close()
        conn.close()

        print("âœ… MariaDB ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print("âŒ DB ì´ˆê¸°í™” ì‹¤íŒ¨:", e)

# âœ… MariaDBì— ê²°ê³¼ ì €ì¥
def save_to_mariadb(results):
    inserted = []
    try:
        conn = mysql.connector.connect(**config)
        cur = conn.cursor()

        for post in results:
            try:
                cur.execute("""
                    INSERT INTO posts (title, author, date, keywords, summary)
                    VALUES (%s, %s, %s, %s, %s)
                    ON DUPLICATE KEY UPDATE
                        keywords = VALUES(keywords),
                        summary = VALUES(summary)
                """, (
                    post["ì œëª©"],
                    post["ì‘ì„±ì"],
                    post["ë‚ ì§œ"],
                    post["í‚¤ì›Œë“œ"],
                    post["ë³¸ë¬¸ ìš”ì•½"]
                ))
                if cur.rowcount == 1:
                    inserted.append(post)
            except Exception as e:
                print("âŒ ì‚½ì… ì˜¤ë¥˜:", e)

        conn.commit()
        cur.close()
        conn.close()
        print("âœ… MariaDB ì €ì¥ ì™„ë£Œ")
        return inserted

    except Exception as e:
        print("âŒ DB ì—°ê²° ì‹¤íŒ¨:", e)
        return []

# âœ… í¬ëŸ¼ í¬ë¡¤ë§ ë° í…”ë ˆê·¸ë¨ ì•Œë¦¼
def crawl_bcw_data():
    proxies = {
        "http": "socks5h://127.0.0.1:9050",
        "https": "socks5h://127.0.0.1:9050"
    }

    base_url = "http://bestteermb42clir6ux7xm76d4jjodh3fpahjqgbddbmfrgp4skg2wqd.onion/"
    forum_path = "viewforum.php?f=30"
    results = []

    for page in range(1):  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 1í˜ì´ì§€ë§Œ
        start = page * 25
        url = f"{base_url}{forum_path}" if page == 0 else f"{base_url}{forum_path}&start={start}"
        print(f"ğŸ“„ í˜ì´ì§€ {page+1} ìš”ì²­ ì¤‘: {url}")

        try:
            res = requests.get(url, proxies=proxies, timeout=20)
            soup = BeautifulSoup(res.text, "html.parser")
            post_links = soup.select("a.topictitle")

            for post in post_links:
                title = post.text.strip()
                href = post.get("href")
                if not href:
                    continue
                post_url = base_url + href.lstrip("./")

                try:
                    post_res = requests.get(post_url, proxies=proxies, timeout=20)
                    post_soup = BeautifulSoup(post_res.text, "html.parser")

                    author_node = post_soup.select_one("p.author strong")
                    author = author_node.get_text(strip=True) if author_node else "(ì‘ì„±ì ì—†ìŒ)"

                    author_tag = post_soup.select_one("p.author")
                    date = author_tag.get_text(strip=True).split("Â»")[-1].strip() if author_tag else "(ë‚ ì§œ ì—†ìŒ)"

                    body_tag = post_soup.select_one("div.postbody")
                    summary = body_tag.get_text(strip=True)[:200] if body_tag else "(ë³¸ë¬¸ ì—†ìŒ)"

                    keywords = []
                    for kw in ["USA", "database", "customer", "SSN", "DOB", "fullz", "MEGA", "BIN", "email", "phone", "txt", "leak"]:
                        if kw.lower() in summary.lower():
                            keywords.append(kw)

                    results.append({
                        "ì œëª©": title,
                        "ì‘ì„±ì": author,
                        "ë‚ ì§œ": date,
                        "í‚¤ì›Œë“œ": ", ".join(keywords),
                        "ë³¸ë¬¸ ìš”ì•½": summary
                    })

                except Exception as e:
                    print("âŒ ê²Œì‹œê¸€ ë¡œë”© ì‹¤íŒ¨:", e)

        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page+1} ìš”ì²­ ì‹¤íŒ¨: {e}")

    new_posts = save_to_mariadb(results)

    if new_posts:
        print(f"ğŸ”” ìƒˆ ê²Œì‹œê¸€ {len(new_posts)}ê±´ ìˆìŒ - í…”ë ˆê·¸ë¨ ì „ì†¡ ì¤‘...")
        for post in new_posts:
            message = (
                f"ğŸ“Œ ì œëª©: {post['ì œëª©']}\n"
                f"ğŸ‘¤ ì‘ì„±ì: {post['ì‘ì„±ì']}\n"
                f"ğŸ“… ë‚ ì§œ: {post['ë‚ ì§œ']}\n"
                f"ğŸ”‘ í‚¤ì›Œë“œ: {post['í‚¤ì›Œë“œ']}\n"
                f"ğŸ“ ìš”ì•½: {post['ë³¸ë¬¸ ìš”ì•½'][:150]}..."
            )
            send_message(message)
    else:
        print("â„¹ï¸ ìƒˆ ê²Œì‹œê¸€ ì—†ìŒ")

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    initialize_mariadb()
    crawl_bcw_data()
(f"ğŸ”” ìƒˆë¡œ ì‚½ì…ëœ ê²Œì‹œê¸€ {len(new_posts)}ê±´ ìˆìŒ")

if __name__ == "__main__":
    initialize_mariadb()
    crawl_bcw_data()